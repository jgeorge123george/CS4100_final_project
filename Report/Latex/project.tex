% One really important thing to note is that Overleaf is simply an online product that enables collaboration for LaTeX files. You may choose to create a LaTeX environment locally on your machine (although...it takes >10 GB of storage). To do the latter, refer to: https://www.latex-project.org/get/
% To learn more about how to use LaTeX and its syntax, check out the Overleaf documentation: https://www.overleaf.com/learn

% Defining a document class is the first line in a LaTeX file. It sets the overall layout and styling of the document.
\documentclass[a4paper, twocolumn]{article}

% These packages enable customization of various aspects of the document. Think of them like "import" statements in Python.
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{floatrow}
\usepackage{layout}
\usepackage{amssymb} 
\usepackage{multirow}
\usepackage{caption}
\geometry{margin=1in}
\usepackage{authblk}
\usepackage{indentfirst}
\usepackage[hidelinks]{hyperref}

% \providecommand defines a new command in LaTeX if it does not exist. This is particularly helpful to do if you find yourself repeating a specific set of commands for your a template.
\providecommand{\keywords}[1]
{
  \small	
  \textbf{\; \textit{Keywords---}} #1
}

% Anything between \begin{document} and \end{document} is what gets printed on the page
\begin{document}

\title{\textbf{\huge{Insert Fancy Project Title}}}
\author{\textbf\large{Tammer Haddad, Joseph George}}
\affil{\textbf{Northeastern University, Boston, 02115}}
\date{\today}

% This is required to print the above 4 lines
\maketitle

% Optional, but useful default command in LaTeX to compose an Abstract section.
\begin{abstract}
This project explores machine learning approaches to handwritten text recognition and prediction. We developed a system that takes in digital text using a pipeline that includes Tesseract OCR, Convolutional Neural Network, and Markov Model. We source training data from EMNIST and MINST datasets. Our CNN model achieves 83.27\% accuracy across 62 character classes.

\end{abstract}\maketitle


% \section command is a section title's style. By default, they are numbered. If you don't want them numbered, add a * like below.
\section*{Introduction}

Even the rise of computers, handwritten notes are still a popular method to write down information for classes, meetings, etc. However, the organization and functionality aspect of written notes is inconvenient. For example, finding notes on a previous subject can be difficult manually but it would be easier if the notes were digitized and searchable. Functionally, digitized text allows for editing and sharing of notes. Traditionally, OCR (Optical Character Recognition) has been used to detect characters through rule-based algorithms. However, with the rise of AI approaches, deep learning and convolutional neural networks have improved the accuracy of OCR software. We took inspiration from this technological advancement to create an integrated pipeline that not only transcribes handwritten notes but also predicts subsequent text, making digitization more efficient and useful.

% Here, I refer to Figure \ref{fig:1} without actually listing the actual figure number in plaintext!

\section{Related Work}
\begin{itemize}
    \item Traditional OCR approaches (rule-based, template matching)
    \item Deep learning for character recognition (LeNet, modern CNN architectures)
    \item Prior work on MNIST/EMNIST classification benchmarks
    \item Language modeling and text prediction (n-grams, Markov models)
    \item Existing end-to-end handwriting recognition systems
\end{itemize}

\section{Preliminaries}
\begin{itemize}
    \item OCR stands for Optical Character Recognition, which is a fancy way to say reading. we take an image of text and convert it into readable characters (text letters). we specifically are only using one to chop up images into individual characters, because we are using a CNN to read the text.
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{hello_split.png}
        \label{fig:hello_split}
    \end{figure}
    \item a CNN (Convolutional Neural Network) is a type of deep learning model that is especially good at processing images. it uses layers of filters to detect patterns in images (edges shapes and textures), then looks at patterns WITHIN the patterns to recognize things like letters and numbers. this can be seen in figure \ref{fig:feature_progression}, where the first layers detect simple edges, and later layers detect more complex features, finalizing in the last layer with character predictions.
    \begin{figure}[h]
        \centering
        \includegraphics[width=1\textwidth]{h_feature_progression.png}
        \caption{CNN Feature Progression Across Layers}
        \label{fig:feature_progression}
    \end{figure}
    \item a Markov Model is a statistical model that predicts the next item in a sequence based only on the current item (or several current items). in our case, we use it to predict the next letter in a sequence of letters based on the previous letters (if you see "good morni\_\_", you can guess the next letters are "ng").
    \item we have images of handwritten text, which we first segment into individual character images using OCR. then each image is fed into our CNN, which outputs several predicted characters. these predicted characters form a sequence that can be formed into sentences or words, which is then processed by our Markov model to predict the most likely next characters based on learned language patterns taken from the internet.
\end{itemize}

\section{Data}
\begin{itemize}
    \item the MNIST dataset is a dataset of handwritten numbers 0 to 9, with 60k training samples and 10k for testing. the images are 28x28 greyscale images of single digits.
    \item the EMNIST dataset is the *E*xtended Mnist dataset. this includes uppercase and lowercaqse letters on top of the MNIST digits, totalling to 62 classes. it has 240k training samples and 40k testing samples, also 28x28 greyscale images of single characters (except these are transformed as well, but more on that later).
    \item in our the Markov model, we used the common crawl dataset, which is a massive dataset of web pages crawled from the internet. we used this because it is nigh infinite and has a wide variety of text. we can also pull it in batches to train our model so we dont have to load the same amount every time.
    \item the EMNIST and MNIST datasets we just got off of torchvision datasets, and for the common crawl dataset we used the publicly available dataset on AWS S3
    \item For training, the EMNISt and MNIST already have training and testing splits, so we used those. for the markov model, we tested on the C4 validation set, which is a held out portion of the common crawl dataset specifrically for testing.
    \item in terms of preprocessing, we didnt need to do anything for MNIST and EMNIST, since theyre very well formatted. common crawl needed a little: we discared docs shorter than 100 chars, stripped whitespace from beginning and end, and joined with newline.
    \item we had no class imbalance issues, since we are using established and well balanced datasets.
\end{itemize}

\section{Methodology}
\begin{itemize}
    \item general pipeline (input image -> OCR -> Preprocess -> CNN -> letter combination -> Markov -> output)
    \item We are using tesseract OCR to take an image of standard handwritten text and turn it into individual character images because our CNN is trained on single characters
    \item Once we have those split characters, we preprocess them by greyscaling and inverting, ressizing by adding borders (to account for non square chars) and scaling to 28x28 pixels. for MNIST we then run it, but for EMNISt we also need to transform the image because EMNIST chars are rotated 90 degrees and flipped
    \item Once we have those character predictions from the cnn, we need to DO something with them. for this we have the markov model we trained on the common crawl dataset
    \item The tesseract is used beforehand (for now) to segment the text images, but then it is brought into a folder. Once there, there is a single python file that runs the pipeline from start to finish. the CNN and Markov models are completely seperate and run individually.
    \item There is the possibliity of combining them by using the context given by the markov model to increase the accuracy of the CNN, but we believe it is better to implement that in the runner script rather than within either model.
\end{itemize}

\section{Experiments}
\begin{itemize}
    \item Optimizer: Adam, Learning rate: 1e-3, Batch size: 32, Loss: CrossEntropyLoss
    \item FOR EPOCHS, We trained 50 MNIST epochs and 20 EMNIST epochs, then saved the model that had the highest test accuracy. for EMNIST this was epoch 3, and MNIST it was 40. after epoch 3-4 of EMNISt, the accuracy decreased we believe due to overfitting (since loss kept decreasing, shown in figures \ref{fig:loss_mnist} and \ref{fig:loss_emnist}).
    \begin{figure}[h]
        \centering
        \begin{floatrow}
            \ffigbox{\includegraphics[width=0.6\textwidth]{loss_mnist.png}}{\caption{MNIST Model Loss}\label{fig:loss_mnist}}
            \ffigbox{\includegraphics[width=0.6\textwidth]{loss_emnist.png}}{\caption{EMNIST Model Loss}\label{fig:loss_emnist}}
        \end{floatrow}
    \end{figure}
    \item There were several hardware and software environments used, sometimes locally on a local machine with an NVIDIA GPU, and sometimes on a remote cluster using a V100 GPU. The code was written in Python 3.8 using PyTorch and torchvision
    \item Our evaluation metrics were primarily accuracy. we chose the epoch level based on highest testing accuracy. We also checked the confusion matrix as seen in figure \ref{fig:confusion_matrix_byclass} to see which characters were being confused the most. 
    \item Using Linear classifiers and other CNNs as a baseline, we compare them to ours. The original EMNIST paper \cite{EMNIST:1} gets 51.80\% from a linear classifier, and 77.57\% with an OPIUM Neural Network with 10k hidden neurons. Other work \cite{ICDAR:2011} has managed 88.12\% using a committee of seven deep CNNs, and we managed 85.90\% accuracy on EMNIST byclass with a single CNN.
    \item We have not yet combined the Markov model with the CNN for the single character prediction with context, we have them working seperately, but hope to combine them in the near future (working versions are already done but not solid).
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.2\textwidth]{confusion_matrix_byclass.png}
    \caption{EMNIST Byclass Confusion Matrix}
    \label{fig:confusion_matrix_byclass}
\end{figure}

\section{Results}
\begin{itemize}
    \item test loader successful: 116323 test samples
Loaded model trained on 'byclass' split with 62 classes
Accuracy on EMNIST byclass Test Set: 85.90%
Correct: 99921/116323
test loader successful: 10000 test samples
Loaded model trained on 'mnist' split with 10 classes
Accuracy on EMNIST mnist Test Set: 99.21%
Correct: 9921/10000

\item Figures \ref{fig:mnist_accuracy} and \ref{fig:byclass_accuracy} show accuracy improvements across training epochs for both datasets.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{mnist_epochs_accuracy.png}
    \caption{MNIST Model Accuracy Over Epochs}
    \label{fig:mnist_accuracy}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{byclass_epochs_accuracy.png}
    \caption{EMNIST Byclass Model Accuracy Over Epochs}
    \label{fig:byclass_accuracy}
\end{figure}
    \item The most common mistakes we see are lowercase characters being confused for their upper case variants (ex: 'O' and 'o', 'I' and 'l' and '1'), which makes sense, since they they often have nearly identical shapes, with size being the differing factor. in fact the lowercase letter 'c' is almost always misclassified as uppercase 'C', showing that size is a major factor in classification.
    \item Markov model prediction accuracy: Top-1 Accuracy: 59.84\%, Top-3 Accuracy: 77.03\%, Top-5 Accuracy: 82.43\%. The more often the charater appears the higher accuracy it predicts it. The model can predict spaces with 88.72\% accuracy, 'e' with 71.87\%, but 'm' for example only 40.12\%.
    \item A good example output:
    \begin{figure}[h]
        \centering
        \includegraphics[width=1\textwidth]{h_classify.png}
        \label{fig:example_output}
    \end{figure}
\end{itemize}

\section{Discussion}
\begin{itemize}
    \item The current main limitations is that messing up a single character completely throws the markov model out of whack, since it relies on the previous characters to predict the next one. so if the CNN misclassifies a character, the markov model will be predicting based on that wrong character, leading to error hell.
    \item State of the art methods (transformers, larger datasets) would vastly improve performance, and even other EMNIST models have been trained with 95.88\% accuracy on the letters, and 91.05\% on balanced \cite{SpinalNet:2023}, which is much higher than ours. They have obviously had a lot more time to fix theirs, so we hope to improve ours in the future as well, or we could pivot to simply using theirs.
    \item Future work: we hope to streamline the process by including the tesseract into the main program (within the github), and make it a web app able to take images directly from users. This could be extended to a video feed as well by taking snapshots. Using the markov model to increase the accuracy of the CNN is also well within possibility, and is already being worked on.
    \item There are quite a few practical applications to the mix of Markov models and CNNs for handwritten text recognition. For one thing there is the easy answer in saying that nearly anything a CNN is used for the markov could be included to improve it's accuracy. for example, license plate recognition could be improved by using a markov model to predict the next character based on state specific license plate formats. We specifically were thinking of handwriting because of note taking and digitization because we are viewing it through the lens of students, but there are swathes of other contexts in which people write things, and this would be useful for all of them.
\end{itemize}

\begin{equation}
    \int_a^b x^2\;\mathrm{d}x= \tfrac{1}{3} x^3 \Big|_a^b
\end{equation}

\section{Github Link \& Team Contributions}
You can find the complete project repository at \href{https://github.com/jgeorge123george/CS4100_final_project}{\texttt{github.com/jgeorge123george/CS4100\_final\_project}}.

\subsection{Team Contributions}
\begin{itemize}
    \item \textbf{Joseph George:} [Specify contributions]
    \item \textbf{Tammer Haddad:} [Specify contributions]
\end{itemize}

\begin{thebibliography}{9}
\bibitem{EMNIST:1}
Cohen, Gregory, et al. "EMNIST: An Extension of MNIST to Handwritten Letters." \textit{arXiv preprint arXiv:1702.05373}, Mar. 2017. https://doi.org/10.48550/arXiv.1702.05373.

\bibitem{ICDAR:2011}
Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. "Convolutional Neural Network Committees for Handwritten Character Classification." \textit{2011 International Conference on Document Analysis and Recognition}, Beijing, China, 2011, pp. 1135--1139. https://doi.org/10.1109/ICDAR.2011.229.

\bibitem{SpinalNet:2023}
Kabir, H M Dipu, et al. "SpinalNet: Deep Neural Network With Gradual Input." \textit{IEEE Transactions on Artificial Intelligence}, vol. 4, no. 5, Oct. 2023, pp. 1165--77. https://doi.org/10.1109/TAI.2022.3185179.


\end{thebibliography}

\end{document}